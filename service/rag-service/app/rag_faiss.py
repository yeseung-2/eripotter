#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RAG ì„ë² ë”© ì„œë¹„ìŠ¤ (FAISS ì „ìš©, ìµœì í™”íŒ)
- document/sr, document/tcfd ë¥¼ ì„ë² ë”©í•˜ì—¬
  service/rag-service/vectordb/<collection> ì•„ë˜ì— FAISS ì¸ë±ìŠ¤ ì €ì¥
- ì„ë² ë”©: HuggingFace (E5-base), GPU(CUDA) ê¸°ë³¸ / ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ ê°€ëŠ¥
- ê±°ë¦¬: ì½”ì‚¬ì¸(COSINE) ê³ ì • (ì •ê·œí™”ëœ E5ì™€ ìµœì  ì¡°í•©)
- Chroma / pgvector ì½”ë“œ ì—†ìŒ
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple

import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS, DistanceStrategy
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader


# ---------- ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ ê²½ë¡œë¡œ ëª…í™•í•˜ê²Œ) ----------
PROJECT_ROOT = Path("C:/taezero/auto_sr")                       # í”„ë¡œì íŠ¸ ë£¨íŠ¸
BASE_PATH = PROJECT_ROOT / "document"                            # ë¬¸ì„œ ë£¨íŠ¸
SR_DIR = BASE_PATH / "sr"
TCFD_DIR = BASE_PATH / "tcfd"
FAISS_DIR = PROJECT_ROOT / "service" / "rag-service" / "vectordb"
FAISS_DIR.mkdir(parents=True, exist_ok=True)

# ---------- ë¡œê¹… ----------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("rag_faiss")

# ---------- ì„ë² ë”© ëª¨ë¸ ----------
EMBED_MODEL_NAME = os.environ.get("EMBED_MODEL_NAME", "intfloat/multilingual-e5-base")
# GPU/CPU ì „í™˜: ê¸°ë³¸ì€ cuda (GPU). GPU ë¶€ì¡±/ì—†ë‹¤ë©´ EMBED_DEVICE=cpu ë¡œ ë°”ê¿” ì‹¤í–‰í•˜ì„¸ìš”.
EMBED_DEVICE = os.environ.get("EMBED_DEVICE", "cuda")  # "cuda" or "cpu"
# ë°°ì¹˜ í¬ê¸°(ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8, 4ë¡œ ë‚®ì¶”ì„¸ìš”)
EMBED_BATCH_SIZE = int(os.environ.get("EMBED_BATCH_SIZE", "16"))

embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": EMBED_DEVICE},
    encode_kwargs={
        "normalize_embeddings": True,   # ì½”ì‚¬ì¸ê³¼ ê¶í•© ìµœì 
        "batch_size": EMBED_BATCH_SIZE,
    },
)

# ---------- í…ìŠ¤íŠ¸ ë¶„í•  ----------
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=150,
    length_function=len,
    separators=["\n## ", "\n#", "\n\n", "\n", " "],
)

# ---------- ìœ í‹¸ ----------
def extract_company_year(filename: str) -> Tuple[str, str]:
    """íŒŒì¼ëª… 'íšŒì‚¬ëª…_ì—°ë„.pdf' í˜•íƒœì—ì„œ íšŒì‚¬/ì—°ë„ ì¶”ì¶œ"""
    name = filename.replace(".pdf", "")
    parts = name.split("_")
    if len(parts) >= 2:
        return parts[0], parts[-1]
    return name, "unknown"

def load_tcfd_mapping() -> Dict[str, Dict[str, str]]:
    """
    TCFD ë§¤í•‘: document/TCFD_MAPPING_FINAL.xlsx ë¥¼ ì½ì–´
    company í‚¤ë¡œ ëŒ€í‘œ ë ˆì½”ë“œë¥¼ 1ê°œì”© ì €ì¥.
    í•„ìš”í•œ í•„ë“œë§Œ ë‚¨ê¸°ê³  ì»¬ëŸ¼ëª… ì •ê·œí™”.
    """
    xls = BASE_PATH / "TCFD_MAPPING_FINAL.xlsx"
    if not xls.exists():
        logger.info("TCFD ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê±´ë„ˆëœ€)")
        return {}
    try:
        df = pd.read_excel(xls)

        # ì—‘ì…€ì—ì„œ ì˜ë¦° ì»¬ëŸ¼ëª… ë³´ì • (ì¼€ì´ìŠ¤ë³„ ë³´ê°•)
        rename_map = {
            '...on': 'description',
            'section': 'tcfd_section' if 'tcfd_section' not in df.columns else 'section',
        }
        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

        # í•„ìš” ì»¬ëŸ¼ í™•ë³´
        needed = ['company', 'year', 'pillar', 'section_tag',
                  'tcfd_section', 'requirement_id', 'page_from', 'page_to']
        for col in needed:
            if col not in df.columns:
                df[col] = None

        # íšŒì‚¬ë³„ ëŒ€í‘œ 1í–‰ (ì—¬ëŸ¬ ê°œë©´ ë§ˆì§€ë§‰)
        df = df.sort_index()
        grouped = df.groupby('company', dropna=True).tail(1)

        mapping: Dict[str, Dict[str, str]] = {}
        for _, r in grouped.iterrows():
            company = str(r.get('company') or '').strip()
            if not company:
                continue
            mapping[company] = {
                'pillar': str(r.get('pillar') or ''),
                'section_tag': str(r.get('section_tag') or ''),
                'tcfd_section': str(r.get('tcfd_section') or ''),
                'requirement_id': str(r.get('requirement_id') or ''),
                'mapped_page_from': str(r.get('page_from') or ''),
                'mapped_page_to': str(r.get('page_to') or ''),
                'mapping_year': str(r.get('year') or ''),
            }
        logger.info(f"TCFD ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(mapping)}ê°œ íšŒì‚¬")
        return mapping
    except Exception as e:
        logger.error(f"TCFD ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def load_pdf_dir(pdf_dir: Path) -> List[Any]:
    """PDF ë””ë ‰í† ë¦¬ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜(+ë©”íƒ€)"""
    docs: List[Any] = []
    if not pdf_dir.exists():
        logger.error(f"PDF ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_dir}")
        return docs

    tcfd_map = load_tcfd_mapping()

    for pdf in sorted(pdf_dir.glob("*.pdf")):
        try:
            logger.info(f"PDF ë¡œë”© ì¤‘: {pdf.name}")
            pages = PyPDFLoader(str(pdf)).load()
            company, year = extract_company_year(pdf.name)
            for i, page in enumerate(pages):
                meta = {
                    "collection": pdf_dir.name,
                    "source": pdf.name,
                    "company": company,
                    "year": year,
                    "page_from": i + 1,
                    "page_to": i + 1,
                    "type": "pdf",
                }
                # ë§¤í•‘ ì •ë³´ ë³‘í•© (ì¡´ì¬ ì‹œ)
                if company in tcfd_map:
                    meta.update(tcfd_map[company])
                page.metadata.update(meta)
            docs.extend(pages)
            logger.info(f"âœ… {pdf.name} ë¡œë”© ì™„ë£Œ ({len(pages)} í˜ì´ì§€)")
        except Exception as e:
            logger.error(f"âŒ {pdf.name} ë¡œë”© ì‹¤íŒ¨: {e}")
    return docs

def split_docs(docs: List[Any]) -> Tuple[List[str], List[Dict[str, Any]]]:
    """LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ í…ìŠ¤íŠ¸/ë©”íƒ€ë¡œ ë³€í™˜"""
    texts, metas = [], []
    for d in docs:
        chunks = text_splitter.split_text(d.page_content or "")
        for c in chunks:
            if not c.strip():
                continue
            texts.append(c)
            metas.append(dict(d.metadata))
    return texts, metas

def upsert_faiss(texts: List[str], metadatas: List[Dict[str, Any]], collection_name: str):
    """FAISS ì¸ë±ìŠ¤ ìƒì„±/ì¶”ê°€ ì €ì¥ (ë¡œì»¬ ì˜ì†, ì½”ì‚¬ì¸ ê±°ë¦¬)"""
    path = FAISS_DIR / collection_name
    path.mkdir(parents=True, exist_ok=True)
    idx_file, pkl_file = path / "index.faiss", path / "index.pkl"

    if idx_file.exists() and pkl_file.exists():
        store = FAISS.load_local(
            str(path),
            embeddings,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.COSINE,
        )
        store.add_texts(texts, metadatas=metadatas)
        store.save_local(str(path))
        logger.info(f"ğŸ” FAISS ì¶”ê°€ ì €ì¥: {collection_name}, +{len(texts)} chunks")
    else:
        store = FAISS.from_texts(
            texts,
            embeddings,
            metadatas=metadatas,
            distance_strategy=DistanceStrategy.COSINE,
        )
        store.save_local(str(path))
        logger.info(f"ğŸ†• FAISS ì‹ ê·œ ìƒì„±: {collection_name}, chunks={len(texts)}")

def build_collection(pdf_dir: Path, collection_name: str, test_query: str):
    docs = load_pdf_dir(pdf_dir)
    if not docs:
        logger.warning(f"ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
        return
    texts, metas = split_docs(docs)
    logger.info(f"âœ… {collection_name}: {len(texts)}ê°œ ì²­í¬ ìƒì„±")
    if not texts:
        logger.warning(f"{collection_name}: ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ìŠ¤í‚µ)")
        return

    upsert_faiss(texts, metas, collection_name)

    # ê°„ë‹¨ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    try:
        path = FAISS_DIR / collection_name
        store = FAISS.load_local(
            str(path),
            embeddings,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.COSINE,
        )
        results = store.similarity_search(test_query, k=3)
        logger.info(f"[ê²€ìƒ‰] '{collection_name}' top-3 for '{test_query}'")
        for i, r in enumerate(results, 1):
            meta = getattr(r, "metadata", {})
            logger.info(f" {i}. {meta.get('source')} p.{meta.get('page_from')} - {r.page_content[:80]}...")
    except Exception as e:
        logger.warning(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    logger.info(f"ğŸš€ FAISS ì„ë² ë”© ì €ì¥ ì‹œì‘ (device={EMBED_DEVICE}, batch_size={EMBED_BATCH_SIZE}, model={EMBED_MODEL_NAME})")
    build_collection(SR_DIR, "sr_corpus", "ê¸°í›„ë³€í™”")
    build_collection(TCFD_DIR, "standards", "ê±°ë²„ë„ŒìŠ¤")
    logger.info("ğŸ‰ ì™„ë£Œ")
    logger.info(f"ğŸ“ ì¸ë±ìŠ¤ ê²½ë¡œ: {FAISS_DIR}\\<collection>\\(index.faiss, index.pkl)")

if __name__ == "__main__":
    main()

