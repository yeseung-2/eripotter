#!/usr/bin/env python3
"""
ê°œì„ ëœ PAIR â†’ Triplet ë³€í™˜
1. ì‹¤ì œ ë§¤í•‘ ê´€ê³„ ê¸°ë°˜ Positive ì„ íƒ
2. ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ë¬¼ì§ˆë“¤ì„ Negativeë¡œ ì„ íƒ
3. ë” ë§ì€ triplet ìƒì„±
"""

import pandas as pd
import numpy as np
import random
import json
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import defaultdict

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
REG_XLSX = "./data/reg_test1.xlsx"
PAIR_CSV = "./data/training_pairs_from_reg_hard5_typos3 (1).csv"
OUTPUT_TRAIN = "./data/triplets_train_improved.jsonl"
OUTPUT_DEV = "./data/triplets_dev_improved.jsonl"

def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ê·œì • ë°ì´í„°
    reg = pd.read_excel(REG_XLSX).fillna("")
    reg.columns = [c.strip().lower() for c in reg.columns]
    reg = reg[["sid", "name"]].drop_duplicates()
    
    # í›ˆë ¨ ìŒ ë°ì´í„° (ì‹¤ì œ ë§¤í•‘ ê´€ê³„)
    pair = pd.read_csv(PAIR_CSV).fillna("")
    pair.columns = [c.strip().lower() for c in pair.columns]
    pair = pair[["raw_name", "standard_substance_id"]].drop_duplicates()
    
    print(f"ê·œì • ë°ì´í„°: {len(reg)}ê°œ")
    print(f"í›ˆë ¨ ìŒ ë°ì´í„°: {len(pair)}ê°œ")
    
    return reg, pair

def build_synonym_groups(reg: pd.DataFrame, pair: pd.DataFrame):
    """ë™ì˜ì–´ ê·¸ë£¹ êµ¬ì¶•"""
    print("ğŸ”„ ë™ì˜ì–´ ê·¸ë£¹ êµ¬ì¶• ì¤‘...")
    
    # SIDë³„ë¡œ ëª¨ë“  ì´ë¦„ ìˆ˜ì§‘
    sid_to_names = defaultdict(set)
    
    # ê·œì • ë°ì´í„°ì—ì„œ ì´ë¦„ ìˆ˜ì§‘
    for _, row in reg.iterrows():
        sid = row["sid"]
        name = row["name"].strip()
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        if (sid and len(str(sid).strip()) > 0 and 
            name and len(name) > 0 and 
            name.lower() != "nan" and 
            not name.startswith("nan")):
            sid_to_names[sid].add(name)
    
    # ë§¤í•‘ ê´€ê³„ì—ì„œë„ ì´ë¦„ ì¶”ê°€
    for _, row in pair.iterrows():
        raw_name = row["raw_name"].strip()
        sid = row["standard_substance_id"]
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        if (sid and len(str(sid).strip()) > 0 and 
            raw_name and len(raw_name) > 0 and 
            raw_name.lower() != "nan" and 
            not raw_name.startswith("nan")):
            sid_to_names[sid].add(raw_name)
    
    # ë™ì˜ì–´ ê·¸ë£¹ ìƒì„± (ìœ íš¨í•œ SIDë§Œ)
    synonym_groups = {}
    for sid, names in sid_to_names.items():
        if len(names) > 0 and sid and len(str(sid).strip()) > 0:
            # ì¶”ê°€ í•„í„°ë§: ë¹ˆ ë¬¸ìì—´ì´ë‚˜ nan ì œê±°
            valid_names = [name for name in names 
                         if name and len(name.strip()) > 0 
                         and name.lower() != "nan" 
                         and not name.startswith("nan")]
            if valid_names:
                synonym_groups[sid] = valid_names
    
    print(f"ë™ì˜ì–´ ê·¸ë£¹: {len(synonym_groups)}ê°œ")
    
    # ê·¸ë£¹ í¬ê¸°ë³„ í†µê³„
    group_sizes = [len(names) for names in synonym_groups.values()]
    print(f"í‰ê·  ê·¸ë£¹ í¬ê¸°: {np.mean(group_sizes):.1f}")
    print(f"ìµœëŒ€ ê·¸ë£¹ í¬ê¸°: {max(group_sizes)}")
    print(f"ìµœì†Œ ê·¸ë£¹ í¬ê¸°: {min(group_sizes)}")
    
    return synonym_groups

def find_similar_substances(synonym_groups: Dict[str, List[str]]):
    """ìœ ì‚¬í•œ ë¬¼ì§ˆë“¤ ì°¾ê¸° (ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ)"""
    print("ğŸ”„ ìœ ì‚¬ ë¬¼ì§ˆ ê·¸ë£¹ êµ¬ì¶• ì¤‘...")
    
    # SIDì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì˜ˆ: GHG-HFCs, APE-VOC ë“±)
    category_to_sids = defaultdict(list)
    for sid in synonym_groups.keys():
        if '__' in sid:
            category = sid.split('__')[0]
            category_to_sids[category].append(sid)
    
    # ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ìœ ì‚¬í•œ ë¬¼ì§ˆë“¤ ì°¾ê¸°
    similar_groups = {}
    for category, sids in category_to_sids.items():
        if len(sids) > 1:
            similar_groups[category] = sids
    
    print(f"ìœ ì‚¬ ë¬¼ì§ˆ ê·¸ë£¹: {len(similar_groups)}ê°œ")
    return similar_groups

def create_improved_triplets(synonym_groups: Dict[str, List[str]], 
                           similar_groups: Dict[str, List[str]],
                           train_ratio: float = 0.8, 
                           max_triplets_per_anchor: int = 30):
    """ê°œì„ ëœ Triplet ìƒì„±"""
    print(f"\nğŸ”„ ê°œì„ ëœ Triplet ìƒì„± ì¤‘... (í›ˆë ¨ ë¹„ìœ¨: {train_ratio*100}%)")
    
    all_sids = list(synonym_groups.keys())
    
    # í›ˆë ¨/ê°œë°œ ë¶„í• 
    random.shuffle(all_sids)
    split_idx = int(len(all_sids) * train_ratio)
    train_sids = all_sids[:split_idx]
    dev_sids = all_sids[split_idx:]
    
    print(f"í›ˆë ¨ SID: {len(train_sids)}ê°œ")
    print(f"ê°œë°œ SID: {len(dev_sids)}ê°œ")
    
    def generate_improved_triplets_for_sids(sids: List[str], max_per_anchor: int = 30):
        triplets = []
        
        for sid in sids:
            names = synonym_groups[sid]
            if len(names) < 1:
                continue
            

            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category = sid.split('__')[0] if '__' in sid else 'unknown'
            
            # ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ SIDë“¤ (ìœ ì‚¬í•œ ë¬¼ì§ˆë“¤)
            similar_sids = similar_groups.get(category, [])
            similar_sids = [s for s in similar_sids if s != sid]
            
            # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì˜ SIDë“¤ (ë‹¤ë¥¸ ë¬¼ì§ˆë“¤)
            other_sids = [s for s in all_sids if s not in similar_sids and s != sid]
            
            for i in range(min(len(names), max_per_anchor)):
                anchor = names[i]
                
                # Positive ì„ íƒ ì „ëµ:
                # 1. ê°™ì€ SIDì˜ ë‹¤ë¥¸ ì´ë¦„ (ê°€ì¥ í™•ì‹¤í•œ ë™ì˜ì–´)
                # 2. ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ SID ì´ë¦„ (ìœ ì‚¬í•œ ë¬¼ì§ˆ)
                positive_candidates = []
                
                # 1. ê°™ì€ SIDì˜ ë‹¤ë¥¸ ì´ë¦„
                same_sid_names = [n for n in names if n != anchor and n.strip()]
                positive_candidates.extend(same_sid_names)
                
                # 2. ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ SID ì´ë¦„ (ê°€ì¤‘ì¹˜ ë‚®ê²Œ)
                if similar_sids:
                    for similar_sid in random.sample(similar_sids, min(3, len(similar_sids))):
                        similar_names = synonym_groups[similar_sid]
                        positive_candidates.extend(similar_names)
                
                # Positive ì„ íƒ
                if positive_candidates:
                    positive = random.choice(positive_candidates)
                else:
                    continue
                
                # Negative ì„ íƒ ì „ëµ:
                # 1. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì˜ ë¬¼ì§ˆë“¤ (ê°€ì¥ í™•ì‹¤í•œ ë‹¤ë¥¸ ë¬¼ì§ˆ)
                # 2. ê°™ì€ ì¹´í…Œê³ ë¦¬ì§€ë§Œ ë‹¤ë¥¸ SID (ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ë¬¼ì§ˆ)
                negative_candidates = []
                
                # 1. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì˜ ë¬¼ì§ˆë“¤
                if other_sids:
                    for other_sid in random.sample(other_sids, min(5, len(other_sids))):
                        other_names = synonym_groups[other_sid]
                        negative_candidates.extend(other_names)
                
                # 2. ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ SID (ê°€ì¤‘ì¹˜ ë‚®ê²Œ)
                if similar_sids:
                    for similar_sid in random.sample(similar_sids, min(2, len(similar_sids))):
                        similar_names = synonym_groups[similar_sid]
                        negative_candidates.extend(similar_names)
                
                # Negative ì„ íƒ
                if negative_candidates:
                    negative = random.choice(negative_candidates)
                else:
                    continue
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê°•í™”
                if (not anchor.strip() or not positive.strip() or not negative.strip() or
                    anchor.lower() == "nan" or positive.lower() == "nan" or negative.lower() == "nan" or
                    anchor.startswith("nan") or positive.startswith("nan") or negative.startswith("nan")):
                    continue
                
                # negativeì˜ ì‹¤ì œ SID ì°¾ê¸°
                negative_sid = None
                for test_sid, test_names in synonym_groups.items():
                    if negative in test_names:
                        negative_sid = test_sid
                        break
                
                # positiveì˜ ì‹¤ì œ SID ì°¾ê¸° (ê°™ì€ SIDê°€ ì•„ë‹Œ ê²½ìš°)
                positive_sid = sid
                if positive not in names:  # ê°™ì€ SIDì— ì—†ëŠ” ê²½ìš°
                    for test_sid, test_names in synonym_groups.items():
                        if positive in test_names:
                            positive_sid = test_sid
                            break
                
                # SID ê²€ì¦ - ëª¨ë“  SIDê°€ ìœ íš¨í•´ì•¼ í•¨
                if not negative_sid or not positive_sid:
                    continue
                
                # ì¹´í…Œê³ ë¦¬ ê²€ì¦
                if category == "unknown" or not category:
                    continue
                
                # Triplet ì¶”ê°€
                triplet = {
                    "anchor": anchor,
                    "positive": positive,
                    "negative": negative,
                    "anchor_sid": sid,
                    "positive_sid": positive_sid,
                    "negative_sid": negative_sid,
                    "category": category
                }
                triplets.append(triplet)
        
        return triplets
    
    # í›ˆë ¨/ê°œë°œ Triplet ìƒì„±
    train_triplets = generate_improved_triplets_for_sids(train_sids, max_triplets_per_anchor)
    dev_triplets = generate_improved_triplets_for_sids(dev_sids, max_triplets_per_anchor)
    
    print(f"í›ˆë ¨ Triplet: {len(train_triplets)}ê°œ")
    print(f"ê°œë°œ Triplet: {len(dev_triplets)}ê°œ")
    
    return train_triplets, dev_triplets

def save_triplets(triplets: List[Dict], output_file: str):
    """Triplet ì €ì¥"""
    print(f"ğŸ’¾ Triplet ì €ì¥ ì¤‘: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for triplet in triplets:
            f.write(json.dumps(triplet, ensure_ascii=False) + '\n')
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {len(triplets)}ê°œ")

def analyze_improved_triplets(train_triplets: List[Dict], dev_triplets: List[Dict]):
    """ê°œì„ ëœ Triplet ë¶„ì„"""
    print("\nğŸ“Š ê°œì„ ëœ Triplet ë¶„ì„...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    train_categories = defaultdict(int)
    dev_categories = defaultdict(int)
    
    for t in train_triplets:
        train_categories[t["category"]] += 1
    
    for t in dev_triplets:
        dev_categories[t["category"]] += 1
    
    print("í›ˆë ¨ ë°ì´í„° ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in sorted(train_categories.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {category}: {count}ê°œ")
    
    print("ê°œë°œ ë°ì´í„° ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in sorted(dev_categories.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {category}: {count}ê°œ")
    
    # ì¤‘ë³µ í™•ì¸
    train_anchors = set(t["anchor"] for t in train_triplets)
    dev_anchors = set(t["anchor"] for t in dev_triplets)
    overlap_anchors = train_anchors.intersection(dev_anchors)
    
    print(f"\nì¤‘ë³µ í™•ì¸:")
    print(f"  - Anchor ì¤‘ë³µ: {len(overlap_anchors)}ê°œ")
    
    if len(overlap_anchors) > 0:
        print("âš ï¸ Anchor ì¤‘ë³µì´ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("âœ… Anchorê°€ ì™„ì „íˆ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ê°œì„ ëœ PAIR â†’ Triplet ë³€í™˜")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    reg, pair = load_data()
    
    # 2. ë™ì˜ì–´ ê·¸ë£¹ êµ¬ì¶•
    synonym_groups = build_synonym_groups(reg, pair)
    
    # 3. ìœ ì‚¬ ë¬¼ì§ˆ ê·¸ë£¹ êµ¬ì¶•
    similar_groups = find_similar_substances(synonym_groups)
    
    # 4. ê°œì„ ëœ Triplet ìƒì„±
    train_triplets, dev_triplets = create_improved_triplets(
        synonym_groups, similar_groups, max_triplets_per_anchor=30
    )
    
    # 5. Triplet ë¶„ì„
    analyze_improved_triplets(train_triplets, dev_triplets)
    
    # 6. ì €ì¥
    save_triplets(train_triplets, OUTPUT_TRAIN)
    save_triplets(dev_triplets, OUTPUT_DEV)
    
    # 7. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ ê°œì„ ëœ Triplet ìƒì„± ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“ í›ˆë ¨ ë°ì´í„°: {OUTPUT_TRAIN}")
    print(f"ğŸ“ ê°œë°œ ë°ì´í„°: {OUTPUT_DEV}")
    print(f"ğŸ“Š ì´ Triplet: {len(train_triplets) + len(dev_triplets)}ê°œ")
    print(f"ğŸ“ˆ ì´ì „ ëŒ€ë¹„ ì¦ê°€: {((len(train_triplets) + len(dev_triplets)) / 312 - 1) * 100:.1f}%")

if __name__ == "__main__":
    main()
